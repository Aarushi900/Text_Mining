{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aarushi900/Text_Mining/blob/main/Lab_sheet_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab Sheet: Text Mining Preprocessing with Python**"
      ],
      "metadata": {
        "id": "v6vZfOHWoYPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "\n",
        "1.   Implement basic text preprocessing techniques.\n",
        "2.Clean and prepare text data for further analysis.\n",
        "2.   Use Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) models to vectorize text data.\n",
        "3.   Understand and apply common text mining techniques.\n",
        "5.Use libraries such as nltk and re to perform common preprocessing tasks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yNCf5ILGod9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nlsL5fRqpJtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prerequisites**\n",
        "\n",
        "*   Basic knowledge of Python programming.\n",
        "*   Understanding of text processing concepts.\n",
        "* Python installed on your machine.\n",
        "* Internet connection to download necessary libraries.\n",
        "\n"
      ],
      "metadata": {
        "id": "m1mtG5nSpfpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions**\n",
        "\n",
        "**1. Set Up Your Environment**\n",
        "1. Install Python: Make sure you have Python 3.x installed. You can download it from python.org.\n",
        "\n",
        "2. Create and Activate a Virtual Environment (optional but recommended):\n",
        "\n",
        "* Create a virtual environment:"
      ],
      "metadata": {
        "id": "6von8yGwpyOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python -m venv myenv"
      ],
      "metadata": {
        "id": "ZImJVyuPp_An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Activate the virtual environment\n",
        "  * On Windows"
      ],
      "metadata": {
        "id": "A3sc6wXOqCNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myenv\\Scripts\\activate"
      ],
      "metadata": {
        "id": "ZodQjtpLqMdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * On macOS/Linux"
      ],
      "metadata": {
        "id": "u4zs7_FIqPL8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgroKgZfoR_O"
      },
      "outputs": [],
      "source": [
        "source myenv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Install Required Libraries:\n",
        "* Install 'nltk'"
      ],
      "metadata": {
        "id": "C6pyMCvsqbWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "VZEqwH-xqiAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e877fd-c7a2-4352-c68f-dc25ddb2e2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Download NLTK Resources**\n",
        "\n",
        "You need to download specific resources from 'nltk' to use stopwords and tokenizers. Run the following code to download these resources:"
      ],
      "metadata": {
        "id": "eqU2lIubql-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "I9REAMWKqrNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9921c7a2-d350-4090-cb9f-527a52ce6408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Implement the Preprocessing Code**\n",
        "\n",
        "Create a Python script or Jupyter Notebook with the following code to perform text preprocessing:"
      ],
      "metadata": {
        "id": "gGblDLqBqthU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and perform stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello world! This is an example of text preprocessing in Python. Let's clean this text.\"\n",
        "\n",
        "# Preprocess the sample text\n",
        "preprocessed_tokens = preprocess_text(text)\n",
        "print(preprocessed_tokens)"
      ],
      "metadata": {
        "id": "94-YPJfyqzxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c258ef-db7b-4a98-8d3e-0d3d844105d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'exampl', 'text', 'preprocess', 'python', 'let', 'clean', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Run the Code**\n",
        "\n",
        "Execute the script or notebook to see the output of the 'preprocessing steps'. The preprocessed_tokens variable should contain the cleaned and processed tokens from the sample text."
      ],
      "metadata": {
        "id": "-ENrOledq1j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Implement Bag of Words (BoW)**\n",
        "\n",
        "Add the following code to convert the preprocessed text into a Bag of Words model:"
      ],
      "metadata": {
        "id": "tUpab2Hrq94G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Hello world! This is an example of text preprocessing.\",\n",
        "    \"Text mining is an important aspect of data analysis.\",\n",
        "    \"Let's clean and prepare text data for analysis.\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "document_matrix = X.toarray()\n",
        "\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"Document Matrix (BoW):\")\n",
        "print(document_matrix)"
      ],
      "metadata": {
        "id": "SwV7ch_5rGD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9a92b5-7bb7-4d79-ecde-35421eb2c90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['analysis' 'aspect' 'clean' 'data' 'example' 'hello' 'important' 'let'\n",
            " 'mining' 'prepare' 'preprocessing' 'text' 'world']\n",
            "Document Matrix (BoW):\n",
            "[[0 0 0 0 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 0 1 0 0 1 0 1 0 0 1 0]\n",
            " [1 0 1 1 0 0 0 1 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Implement TF-IDF**\n",
        "\n",
        "Add the following code to convert the text into a TF-IDF representation:"
      ],
      "metadata": {
        "id": "rI7rU_V0rK8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "document_matrix_tfidf = X_tfidf.toarray()\n",
        "\n",
        "print(\"Feature Names (TF-IDF):\", feature_names_tfidf)\n",
        "print(\"Document Matrix (TF-IDF):\")\n",
        "print(document_matrix_tfidf)"
      ],
      "metadata": {
        "id": "7kR0SfdlrI5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2f825a-da7e-4bc9-da01-864cddc2007c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (TF-IDF): ['analysis' 'aspect' 'clean' 'data' 'example' 'hello' 'important' 'let'\n",
            " 'mining' 'prepare' 'preprocessing' 'text' 'world']\n",
            "Document Matrix (TF-IDF):\n",
            "[[0.         0.         0.         0.         0.47952794 0.47952794\n",
            "  0.         0.         0.         0.         0.47952794 0.28321692\n",
            "  0.47952794]\n",
            " [0.35829137 0.4711101  0.         0.35829137 0.         0.\n",
            "  0.4711101  0.         0.4711101  0.         0.         0.27824521\n",
            "  0.        ]\n",
            " [0.35829137 0.         0.4711101  0.35829137 0.         0.\n",
            "  0.         0.4711101  0.         0.4711101  0.         0.27824521\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercises***\n",
        "\n",
        "* Implement Lemmatization: Replace the stemming process with lemmatization using WordNetLemmatizer.\n",
        "* Handle Different Languages: Modify the code to preprocess text in a different language using the appropriate stopwords and tokenizers.\n",
        "\n",
        "* Explore Other Vectorizers: Try using HashingVectorizer and compare it with CountVectorizer and TfidfVectorizer."
      ],
      "metadata": {
        "id": "EWQ96dS8rUeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# Define function to lemmatize each word with its POS tag\n",
        "\n",
        "# POS_TAGGER_FUNCTION : TYPE 1\n",
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Tokenize and POS tagging\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Print tokens and POS tags for debugging\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "    # Lemmatize based on POS tag\n",
        "    lemmatized_tokens = [\n",
        "        lemmatizer.lemmatize(token, pos_tagger(pos)) for token, pos in pos_tags if token.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Print lemmatized tokens for debugging\n",
        "    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Example usage\n",
        "text = \"The running cats are running swiftly.\"\n",
        "print(\"Lemmatized Text:\", lemmatize_text(text))\n"
      ],
      "metadata": {
        "id": "IlECYKpWK1mH",
        "outputId": "85c8b4b0-3c20-4570-f43e-8b4334b6be2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['the', 'running', 'cats', 'are', 'running', 'swiftly', '.']\n",
            "POS Tags: [('the', 'DT'), ('running', 'NN'), ('cats', 'NNS'), ('are', 'VBP'), ('running', 'VBG'), ('swiftly', 'RB'), ('.', '.')]\n",
            "Lemmatized Tokens: ['the', 'running', 'cat', 'be', 'run', 'swiftly']\n",
            "Lemmatized Text: the running cat be run swiftly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "!python -m spacy download fr_core_news_sm\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load French tokenizer and lemmatizer\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "def preprocess_french_text(text):\n",
        "    stop_words = set(sw.words('french'))\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if token.text.lower() not in stop_words and token.is_alpha]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Example usage\n",
        "french_text = \"Les chats courent rapidement.\"\n",
        "print(preprocess_french_text(french_text))  # Output: \"chat courir rapidement\"\n"
      ],
      "metadata": {
        "id": "CvvdJDfHK-xz",
        "outputId": "da6f6c8c-ced6-4b73-cbfb-0e22732c6bac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat courir rapidement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat is on the mat.\",\n",
        "    \"The dog is on the rug.\",\n",
        "    \"Cats and dogs are pets.\"\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform(documents)\n",
        "print(\"CountVectorizer:\\n\", count_vectorizer.get_feature_names_out())\n",
        "print(count_matrix.toarray())\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "print(\"\\nTfidfVectorizer:\\n\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# HashingVectorizer\n",
        "hashing_vectorizer = HashingVectorizer(n_features=10)  # n_features is a parameter to adjust the number of features\n",
        "hashing_matrix = hashing_vectorizer.fit_transform(documents)\n",
        "print(\"\\nHashingVectorizer:\\n\", hashing_matrix.toarray())\n",
        "\n",
        "# Comparing similarity between first two documents\n",
        "similarity_count = cosine_similarity(count_matrix[0:1], count_matrix[1:2])\n",
        "similarity_tfidf = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "similarity_hashing = cosine_similarity(hashing_matrix[0:1], hashing_matrix[1:2])\n",
        "\n",
        "print(\"\\nCosine Similarity CountVectorizer:\", similarity_count)\n",
        "print(\"Cosine Similarity TfidfVectorizer:\", similarity_tfidf)\n",
        "print(\"Cosine Similarity HashingVectorizer:\", similarity_hashing)\n"
      ],
      "metadata": {
        "id": "ccbQ2Il0LB9t",
        "outputId": "07c39eae-e813-43f3-a8b4-2491f80b38fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer:\n",
            " ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'is' 'mat' 'on' 'pets' 'rug' 'the']\n",
            "[[0 0 1 0 0 0 1 1 1 0 0 2]\n",
            " [0 0 0 0 1 0 1 0 1 0 1 2]\n",
            " [1 1 0 1 0 1 0 0 0 1 0 0]]\n",
            "\n",
            "TfidfVectorizer:\n",
            " ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'is' 'mat' 'on' 'pets' 'rug' 'the']\n",
            "[[0.         0.         0.42755362 0.         0.         0.\n",
            "  0.32516555 0.42755362 0.32516555 0.         0.         0.6503311 ]\n",
            " [0.         0.         0.         0.         0.42755362 0.\n",
            "  0.32516555 0.         0.32516555 0.         0.42755362 0.6503311 ]\n",
            " [0.4472136  0.4472136  0.         0.4472136  0.         0.4472136\n",
            "  0.         0.         0.         0.4472136  0.         0.        ]]\n",
            "\n",
            "HashingVectorizer:\n",
            " [[ 0.         -0.40824829  0.          0.          0.          0.\n",
            "   0.          0.40824829 -0.81649658  0.        ]\n",
            " [ 0.         -0.35355339  0.         -0.35355339  0.          0.\n",
            "   0.          0.35355339 -0.70710678  0.35355339]\n",
            " [-0.4472136  -0.4472136   0.          0.4472136   0.         -0.4472136\n",
            "   0.4472136   0.          0.          0.        ]]\n",
            "\n",
            "Cosine Similarity CountVectorizer: [[0.75]]\n",
            "Cosine Similarity TfidfVectorizer: [[0.6343958]]\n",
            "Cosine Similarity HashingVectorizer: [[0.8660254]]\n"
          ]
        }
      ]
    }
  ]
}